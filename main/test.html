<!--
  This demo is pure javascript with all libraries externally fetched. No closure.
-->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@0.1/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.0.1613083229/face_detection.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@0.1/drawing_utils.js" crossorigin="anonymous"></script>


</head>

<body>
<div class="container">
  <video class="input_video"></video>
  <canvas class="output_canvas" width="620px" height="465px"></canvas>
</div>
</body>
</html>

<script type="module">

  var width = 376;
  var height = 301;
  //results.detections[0].boundingBox.height
  //height, width, xCenter, yCenter, filling

  // Our input frames will come from here.
  const videoElement = document.getElementsByClassName('input_video')[0];
  const canvasElement = document.getElementsByClassName('output_canvas')[0];
  const canvasCtx = canvasElement.getContext('2d');


  var canvasxCenter = canvasElement.width / 2;
  var canvasyCenter = canvasElement.height / 2;

  var xCenterTemp;
  var yCenterTemp;
  // processing and visualizing the video input
  function onResults(results) {
    var difx = Math.abs(xCenterTemp - results.detections[0].boundingBox.xCenter);
    var dify = Math.abs(yCenterTemp - results.detections[0].boundingBox.yCenter);
    //checking the intensity of movement
    if (difx < 0.0025) {
      results.detections[0].boundingBox.xCenter = xCenterTemp;
    }
    if (dify < 0.0025) {
      results.detections[0].boundingBox.yCenter = yCenterTemp;
    }
    //calculating location of the face
    var newX = canvasxCenter - results.detections[0].boundingBox.xCenter * canvasElement.width;
    var newY = canvasyCenter - results.detections[0].boundingBox.yCenter * canvasElement.height;


    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

    if (results.detections.length > 0) {
      canvasCtx.translate(canvasElement.width / 2 - 580, canvasElement.height / 2 - 580);
      canvasCtx.scale(3, 2);
      canvasCtx.drawImage(
              results.image, newX * 3, newY * 2, canvasElement.width, canvasElement.height);


      /*drawRectangle(
              canvasCtx, results.detections[0].boundingBox,
              {color: 'blue', lineWidth: 4, fillColor: '#00000000'});
      drawLandmarks(canvasCtx, results.detections[0].landmarks, {
        color: 'red',
        radius: 5,
      });*/
    }
    canvasCtx.restore();
    //saving coordinate values to compare movement
    xCenterTemp = results.detections[0].boundingBox.xCenter;
    yCenterTemp = results.detections[0].boundingBox.yCenter;
  }

  const faceDetection = new FaceDetection({locateFile: (file) => {
      return `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.0.1613083229/${file}`;
    }});
  faceDetection.onResults(onResults);

  // Instantiate a camera. We'll feed each frame we receive into the solution.
  const camera = new Camera(videoElement, {
    onFrame: async () => {
      await faceDetection.send({image: videoElement});
    },
    width: 1240,
    height: 930
  });
  camera.start();
</script>
