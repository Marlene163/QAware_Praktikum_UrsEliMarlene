<!--
  This demo is pure javascript with all libraries externally fetched. No closure.
-->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@0.1/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.0.1613083229/face_detection.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@0.1/drawing_utils.js" crossorigin="anonymous"></script>


</head>

<body>
<div class="container">
  <video class="input_video"></video>
  <canvas class="output_canvas" width="1240px" height="900px"></canvas>
</div>
</body>
</html>

<script type="module">

  var widthCam = 1240;
  var heightCam = 720;

  var scale = 1.5;

  //results.detections[0].boundingBox.height
  //height, width, xCenter, yCenter, filling

  // Our input frames will come from here.
  const videoElement = document.getElementsByClassName('input_video')[0];
  const canvasElement = document.getElementsByClassName('output_canvas')[0];
  const canvasCtx = canvasElement.getContext('2d');




  var xCenterTemp;
  var yCenterTemp;
  // processing and visualizing the video input
  function onResults(results) {
    canvasElement.width = widthCam / 2;
    canvasElement.height = heightCam / 2;

    const canvasxCenter = canvasElement.width / 2;
    const canvasyCenter = canvasElement.height / 2;

    var difx = Math.abs(xCenterTemp - results.detections[0].boundingBox.xCenter);
    var dify = Math.abs(yCenterTemp - results.detections[0].boundingBox.yCenter);
    //stabilizing the video
    if (difx < 0.0025) {
      results.detections[0].boundingBox.xCenter = xCenterTemp;
    }
    if (dify < 0.0025) {
      results.detections[0].boundingBox.yCenter = yCenterTemp;
    }
    //calculating location of the face
    var deltaX = (results.detections[0].boundingBox.xCenter - 0.5) * canvasElement.width * scale;
    var deltaY = (results.detections[0].boundingBox.yCenter - 0.5) * canvasElement.height * scale;


    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

    //moving and zooming the frame to show the face centered
    if (results.detections.length > 0) {
      // moving the picture
      canvasCtx.translate(canvasElement.width / 2 - deltaX,
        canvasElement.height / 2 - deltaY);
      //scaling it up
      canvasCtx.scale(scale, scale);
      //drawing it on the canvas
      canvasCtx.drawImage(
              results.image, -canvasElement.width / 2, -canvasElement.height / 2, canvasElement.width, canvasElement.height);

    }
    canvasCtx.restore();
    //saving coordinate values to compare movement
    xCenterTemp = results.detections[0].boundingBox.xCenter;
    yCenterTemp = results.detections[0].boundingBox.yCenter;
  }

  const faceDetection = new FaceDetection({locateFile: (file) => {
      return `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.0.1613083229/${file}`;
    }});
  faceDetection.onResults(onResults);

  // Instantiate a camera. We'll feed each frame we receive into the solution.
  const camera = new Camera(videoElement, {
    onFrame: async () => {
      await faceDetection.send({image: videoElement});
    },
    width: 1240,
    height: 720
  });
  videoElement.addEventListener( "loadeddata", function (e) {
    widthCam = this.videoWidth;
    heightCam = this.videoHeight;
    console.log(widthCam + " " + heightCam);
  }, false );

  camera.start();
</script>
